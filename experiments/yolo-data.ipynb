{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Multiple YOLO v11 Datasets into One\n",
    "\n",
    "This notebook provides a step-by-step guide to merging multiple YOLO v11 datasets into a single dataset. It ensures that class labels are uniquely mapped to avoid overlaps and maintains the required directory structure.\n",
    "\n",
    "## **Overview**\n",
    "\n",
    "1. **Setup and Imports**\n",
    "2. **Utility Functions**\n",
    "3. **Discovering Datasets**\n",
    "4. **Collecting Unique Classes**\n",
    "5. **Merging Datasets**\n",
    "6. **Generating Final `data.yaml`**\n",
    "7. **Conclusion**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in datasets/broken-pole to yolov11:: 100%|██████████| 11588/11588 [00:00<00:00, 82990.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to datasets/broken-pole in yolov11:: 100%|██████████| 412/412 [00:00<00:00, 14004.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'broken-pole' downloaded to: datasets/broken-pole\n",
      "loading Roboflow workspace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in datasets/mytest-hrswj to yolov11:: 100%|██████████| 106200/106200 [00:02<00:00, 53091.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to datasets/mytest-hrswj in yolov11:: 100%|██████████| 4780/4780 [00:00<00:00, 16166.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'mytest-hrswj' downloaded to: datasets/mytest-hrswj\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in datasets/flood-september-23-dataset to yolov11:: 100%|██████████| 146635/146635 [00:02<00:00, 67407.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to datasets/flood-september-23-dataset in yolov11:: 100%|██████████| 4275/4275 [00:00<00:00, 12482.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'flood-september-23-dataset' downloaded to: datasets/flood-september-23-dataset\n",
      "loading Roboflow workspace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in datasets/fallen_trees_improve_1-1185-dkpus-5ufzr to yolov11:: 100%|██████████| 68223/68223 [00:00<00:00, 86315.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to datasets/fallen_trees_improve_1-1185-dkpus-5ufzr in yolov11:: 100%|██████████| 1779/1779 [00:00<00:00, 12688.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'fallen_trees_improve_1-1185-dkpus-5ufzr' downloaded to: datasets/fallen_trees_improve_1-1185-dkpus-5ufzr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Create a 'datasets' folder if it doesn't exist\n",
    "datasets_folder = \"datasets\"\n",
    "os.makedirs(datasets_folder, exist_ok=True)\n",
    "\n",
    "rf = Roboflow(api_key=os.getenv(\"ROBOFLOW_API_KEY\"))\n",
    "\n",
    "# Download and save datasets\n",
    "datasets = [\n",
    "    (\"ghost-gsj7h\", \"broken-pole\", 3),\n",
    "    (\"mytyolov8\", \"mytest-hrswj\", 2),\n",
    "    (\"september-23-2024-flood-dataset\", \"flood-september-23-dataset\", 1),\n",
    "    (\"hackathons-xbnwb\", \"fallen_trees_improve_1-1185-dkpus-5ufzr\", 1)\n",
    "]\n",
    "\n",
    "for workspace, project_name, version_number in datasets:\n",
    "    project = rf.workspace(workspace).project(project_name)\n",
    "    version = project.version(version_number)\n",
    "    dataset_path = os.path.join(datasets_folder, project_name)\n",
    "    dataset = version.download(\"yolov11\", location=dataset_path)\n",
    "    print(f\"Dataset '{project_name}' downloaded to: {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "We'll define helper functions to load and save YAML files, handle paths, and manage warnings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml(yaml_path):\n",
    "    \"\"\"\n",
    "    Load a YAML file and return its contents.\n",
    "    \"\"\"\n",
    "    with open(yaml_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "def save_yaml(data, yaml_path):\n",
    "    \"\"\"\n",
    "    Save data to a YAML file.\n",
    "    \"\"\"\n",
    "    with open(yaml_path, 'w') as file:\n",
    "        yaml.dump(data, file, sort_keys=False)\n",
    "\n",
    "def get_all_datasets(datasets_root):\n",
    "    \"\"\"\n",
    "    Discover all dataset directories within the root datasets directory.\n",
    "    Assumes each subdirectory is a separate dataset.\n",
    "    \"\"\"\n",
    "    datasets = [os.path.join(datasets_root, d) for d in os.listdir(datasets_root) \n",
    "                if os.path.isdir(os.path.join(datasets_root, d))]\n",
    "    print(f\"Found {len(datasets)} datasets to merge.\")\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Unique Classes\n",
    "\n",
    "To ensure that class labels are unique across all datasets, we'll collect all unique class names and assign new unique IDs to each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_unique_classes(datasets):\n",
    "    \"\"\"\n",
    "    Collect all unique class names across all datasets and assign new unique IDs.\n",
    "\n",
    "    Args:\n",
    "        datasets (list): List of dataset directory paths.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping from class names to unique IDs.\n",
    "    \"\"\"\n",
    "    unique_classes = []\n",
    "    for dataset in datasets:\n",
    "        data_yaml_path = os.path.join(dataset, 'data.yaml')\n",
    "        if not os.path.exists(data_yaml_path):\n",
    "            print(f\"Warning: data.yaml not found in {dataset}. Skipping.\")\n",
    "            continue\n",
    "        data = load_yaml(data_yaml_path)\n",
    "        classes = data.get('names', [])\n",
    "        for cls in classes:\n",
    "            if cls not in unique_classes:\n",
    "                unique_classes.append(cls)\n",
    "    class_to_id = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "    print(f\"Total unique classes collected: {len(class_to_id)}\")\n",
    "    return class_to_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Datasets\n",
    "\n",
    "This section handles the merging of images and labels from all datasets into the final dataset structure. It ensures that class IDs are remapped to the new unique IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(datasets_root, output_root, class_to_id):\n",
    "    \"\"\"\n",
    "    Merge multiple YOLO v11 datasets into a single dataset.\n",
    "\n",
    "    Args:\n",
    "        datasets_root (str): Path to the folder containing multiple datasets.\n",
    "        output_root (str): Path to the output merged dataset folder.\n",
    "        class_to_id (dict): Mapping from class names to unique IDs.\n",
    "    \"\"\"\n",
    "    datasets = get_all_datasets(datasets_root)\n",
    "    if not datasets:\n",
    "        print(\"No datasets found to merge.\")\n",
    "        return\n",
    "    \n",
    "    # Prepare output directories\n",
    "    splits = ['train', 'valid']\n",
    "    output_dirs = {}\n",
    "    for split in splits:\n",
    "        images_path = os.path.join(output_root, split, 'images')\n",
    "        labels_path = os.path.join(output_root, split, 'labels')\n",
    "        os.makedirs(images_path, exist_ok=True)\n",
    "        os.makedirs(labels_path, exist_ok=True)\n",
    "        output_dirs[split] = {'images': images_path, 'labels': labels_path}\n",
    "    \n",
    "    # To handle potential filename conflicts, keep track of filenames\n",
    "    existing_filenames = set()\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        print(f\"\\nMerging dataset: {dataset}\")\n",
    "        data_yaml_path = os.path.join(dataset, 'data.yaml')\n",
    "        data = load_yaml(data_yaml_path)\n",
    "        dataset_classes = data.get('names', [])\n",
    "        if not dataset_classes:\n",
    "            print(f\"Warning: No class names found in {data_yaml_path}. Skipping this dataset.\")\n",
    "            continue\n",
    "        # Create mapping from old class IDs to new class IDs\n",
    "        old_to_new = {str(idx): class_to_id[cls] for idx, cls in enumerate(dataset_classes)}\n",
    "        \n",
    "        # Directories to process: train, valid, test (if exists)\n",
    "        for split in ['train', 'valid', 'test']:\n",
    "            split_dir = os.path.join(dataset, split)\n",
    "            if not os.path.exists(split_dir):\n",
    "                if split != 'test':  # 'test' is optional\n",
    "                    print(f\"Warning: {split_dir} does not exist. Skipping this split.\")\n",
    "                continue\n",
    "            images_dir = os.path.join(split_dir, 'images')\n",
    "            labels_dir = os.path.join(split_dir, 'labels')\n",
    "            if not os.path.exists(images_dir) or not os.path.exists(labels_dir):\n",
    "                print(f\"Warning: Missing images or labels in {split_dir}. Skipping this split.\")\n",
    "                continue\n",
    "            \n",
    "            # Determine target split\n",
    "            target_split = 'valid' if split == 'test' else split\n",
    "            target_images = output_dirs[target_split]['images']\n",
    "            target_labels = output_dirs[target_split]['labels']\n",
    "            \n",
    "            # Process images\n",
    "            for img_name in os.listdir(images_dir):\n",
    "                src_img_path = os.path.join(images_dir, img_name)\n",
    "                if not os.path.isfile(src_img_path):\n",
    "                    continue\n",
    "                # Handle duplicate filenames by renaming\n",
    "                base_name, ext = os.path.splitext(img_name)\n",
    "                new_img_name = img_name\n",
    "                counter = 1\n",
    "                while new_img_name in existing_filenames:\n",
    "                    new_img_name = f\"{base_name}_{counter}{ext}\"\n",
    "                    counter += 1\n",
    "                existing_filenames.add(new_img_name)\n",
    "                dst_img_path = os.path.join(target_images, new_img_name)\n",
    "                shutil.copy2(src_img_path, dst_img_path)\n",
    "                \n",
    "                # Process corresponding label\n",
    "                label_name = base_name + '.txt'\n",
    "                src_label_path = os.path.join(labels_dir, label_name)\n",
    "                if os.path.exists(src_label_path):\n",
    "                    with open(src_label_path, 'r') as f:\n",
    "                        lines = f.readlines()\n",
    "                    new_label_lines = []\n",
    "                    for line in lines:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) < 1:\n",
    "                            continue\n",
    "                        old_class = parts[0]\n",
    "                        if old_class not in old_to_new:\n",
    "                            print(f\"Warning: Class {old_class} not found in mapping for {dataset}. Skipping.\")\n",
    "                            continue\n",
    "                        new_class = old_to_new[old_class]\n",
    "                        new_line = ' '.join([str(new_class)] + parts[1:])\n",
    "                        new_label_lines.append(new_line)\n",
    "                    # Write to new label file with the new image name\n",
    "                    new_label_name = os.path.splitext(new_img_name)[0] + '.txt'\n",
    "                    dst_label_path = os.path.join(target_labels, new_label_name)\n",
    "                    with open(dst_label_path, 'w') as f:\n",
    "                        f.write('\\n'.join(new_label_lines) + '\\n')\n",
    "                else:\n",
    "                    print(f\"Warning: Label file {src_label_path} does not exist for image {src_img_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Final `data.yaml`\n",
    "\n",
    "After merging all datasets, we'll create a final `data.yaml` file that points to the new training and validation image directories and includes the consolidated list of class names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_yaml(output_root, class_to_id):\n",
    "    \"\"\"\n",
    "    Generate the final data.yaml file for the merged dataset.\n",
    "\n",
    "    Args:\n",
    "        output_root (str): Path to the output merged dataset folder.\n",
    "        class_to_id (dict): Mapping from class names to unique IDs.\n",
    "    \"\"\"\n",
    "    final_data_yaml = {\n",
    "        'train': 'train/images',\n",
    "        'val': 'valid/images',\n",
    "        'nc': len(class_to_id),\n",
    "        'names': list(class_to_id.keys())\n",
    "    }\n",
    "    # Save data.yaml\n",
    "    data_yaml_path = os.path.join(output_root, 'data.yaml')\n",
    "    save_yaml(final_data_yaml, data_yaml_path)\n",
    "    print(f\"\\nFinal data.yaml created at {data_yaml_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Merge Process\n",
    "\n",
    "Specify the paths to your datasets and the desired output directory. Then, execute the merging process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 datasets to merge.\n",
      "Total unique classes collected: 5\n",
      "Found 4 datasets to merge.\n",
      "\n",
      "Merging dataset: datasets/mytest-hrswj\n",
      "\n",
      "Merging dataset: datasets/flood-september-23-dataset\n",
      "\n",
      "Merging dataset: datasets/broken-pole\n",
      "\n",
      "Merging dataset: datasets/fallen_trees_improve_1-1185-dkpus-5ufzr\n",
      "\n",
      "Final data.yaml created at final_dataset/data.yaml\n",
      "\n",
      "Merging process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define the root directory containing all individual datasets\n",
    "datasets_root = 'datasets'  # Replace with your datasets folder path\n",
    "\n",
    "# Define the output directory for the merged dataset\n",
    "output_root = 'final_dataset'  # Replace with your desired output path\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "# Step 1: Discover all datasets\n",
    "datasets = get_all_datasets(datasets_root)\n",
    "\n",
    "# Step 2: Collect all unique classes\n",
    "class_to_id = collect_unique_classes(datasets)\n",
    "\n",
    "# Step 3: Merge datasets\n",
    "merge_datasets(datasets_root, output_root, class_to_id)\n",
    "\n",
    "# Step 4: Generate final data.yaml\n",
    "generate_final_yaml(output_root, class_to_id)\n",
    "\n",
    "print(\"\\nMerging process completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calhacks24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
